import streamlit as st
import pandas as pd
import sys
import os
import time
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from utils.data_utils import (
    load_questions, load_question_sets, add_test_result, load_api_presets
)
from utils.openai_utils import (
    evaluate_answer, generate_example_answer_with_llm # Client viene creato internamente
)
from utils.ui_utils import add_page_header, add_section_title, create_card
from utils.bm25 import (
    calcola_similarita_normalizzata, analizza_parole_chiave, 
    genera_suggerimenti, segmenta_testo
)

add_page_header(
    "Esecuzione Test",
    icon="üß™",
    description="Esegui valutazioni automatiche sui tuoi set di domande utilizzando i preset API configurati."
)

# Carica i preset API e verifica che ce ne sia almeno uno
if 'api_presets' not in st.session_state:
    st.session_state.api_presets = load_api_presets()

if st.session_state.api_presets.empty:
    st.error("Nessun preset API configurato. Vai alla pagina 'Gestione Preset API' per crearne almeno uno prima di eseguire i test.")
    st.stop()

# Controlla se ci sono set di domande disponibili
if 'question_sets' not in st.session_state or st.session_state.question_sets.empty:
    st.warning("Nessun set di domande disponibile. Crea dei set di domande prima di eseguire i test.")
    st.stop()

# Ottieni testo della domanda e risposta attesa per ID
def get_question_data(question_id):
    if 'questions' in st.session_state and not st.session_state.questions.empty:
        question_row = st.session_state.questions[st.session_state.questions['id'] == str(question_id)]
        if not question_row.empty:
            # Assicurati che i nomi delle colonne siano corretti (es. 'domanda', 'risposta_attesa')
            # Questi dovrebbero corrispondere a come sono salvati/caricati in data_utils.py
            q = question_row.iloc[0].get('domanda', question_row.iloc[0].get('question', ''))
            a = question_row.iloc[0].get('risposta_attesa', question_row.iloc[0].get('expected_answer', ''))
            
            # Verifica che domanda e risposta non siano vuote
            if not q or not isinstance(q, str) or q.strip() == "":
                st.error(f"La domanda con ID {question_id} √® vuota o non valida.")
                return None
                
            if not a or not isinstance(a, str) or a.strip() == "":
                st.warning(f"La risposta attesa per la domanda con ID {question_id} √® vuota o non valida.")
                # Continuiamo comunque ma con una risposta vuota
                a = "Risposta non disponibile"
                
            return {'question': q, 'expected_answer': a}
    return None

# Seleziona set di domande per il test
add_section_title("Seleziona Set di Domande", icon="üìö")
set_options = {}
if 'question_sets' in st.session_state and not st.session_state.question_sets.empty:
    for _, row in st.session_state.question_sets.iterrows():
        if 'questions' in row and row['questions']:
            set_options[row['id']] = f"{row['name']} ({len(row['questions'])} domande)"

if not set_options:
    st.warning("Nessun set di domande con domande associate. Creane uno in 'Gestione Set di Domande'.")
    st.stop()

selected_set_id = st.selectbox(
    "Seleziona un set di domande", 
    options=list(set_options.keys()),
    format_func=lambda x: set_options[x],
    key="select_question_set_for_test"
)

selected_set = st.session_state.question_sets[st.session_state.question_sets['id'] == selected_set_id].iloc[0]
questions_in_set = selected_set['questions']

# --- Selezione Modalit√† Test (UI Migliorata) ---
st.markdown("## üìå Seleziona modalit√† test")
if "test_mode" not in st.session_state: st.session_state.test_mode = "Valutazione Automatica con LLM"

selection_container = st.container()
with selection_container:
    st.markdown("""<style>...</style>""", unsafe_allow_html=True) # CSS omesso per brevit√†
    col_sel1, col_sel2 = st.columns(2)
    current_mode_sel = st.session_state.test_mode
    
    with col_sel1:
        is_auto_sel = current_mode_sel == "Valutazione Automatica con LLM"
        auto_box_class = "selected-mode" if is_auto_sel else "unselected-mode" # Definizione corretta
        st.markdown(f'''
        <div class="mode-box {auto_box_class}">
            <div class="mode-title">ü§ñ Valutazione Automatica con LLM</div>
            <div class="mode-description">Genera e valuta risposte con modelli linguistici avanzati</div>
        </div>
        ''', unsafe_allow_html=True)
        if st.button("‚úì MODALIT√Ä ATTIVA" if is_auto_sel else "‚úÖ SELEZIONA", key="llm_mode_btn", use_container_width=True, type="primary" if is_auto_sel else "secondary"):
            if not is_auto_sel: 
                st.session_state.test_mode = "Valutazione Automatica con LLM"
                st.rerun()
    with col_sel2:
        is_bm25_sel = current_mode_sel == "Valutazione con BM25"
        bm25_box_class = "selected-mode" if is_bm25_sel else "unselected-mode" # Definizione corretta
        st.markdown(f'''
        <div class="mode-box {bm25_box_class}">
            <div class="mode-title">üîç Valutazione con BM25</div>
            <div class="mode-description">Analizza la somiglianza semantica basata sui termini condivisi</div>
        </div>
        ''', unsafe_allow_html=True)
        if st.button("‚úì MODALIT√Ä ATTIVA" if is_bm25_sel else "‚úÖ SELEZIONA", key="bm25_mode_btn", use_container_width=True, type="primary" if is_bm25_sel else "secondary"):
            if not is_bm25_sel:
                st.session_state.test_mode = "Valutazione con BM25"
                st.rerun()

mode_icon_sel = "ü§ñ" if st.session_state.test_mode == "Valutazione Automatica con LLM" else "üîç"
st.markdown(f'<div style="..."> {mode_icon_sel} MODALIT√Ä SELEZIONATA: {st.session_state.test_mode}</div>', unsafe_allow_html=True)
st.markdown("---")

# --- Opzioni API basate su Preset ---
add_section_title("Opzioni API basate su Preset", icon="üõ†Ô∏è")

preset_names_to_id = {preset['name']: preset['id'] for _, preset in st.session_state.api_presets.iterrows()}
preset_display_names = list(preset_names_to_id.keys())

def get_preset_config_by_name(name):
    preset_id = preset_names_to_id.get(name)
    if preset_id:
        return st.session_state.api_presets[st.session_state.api_presets["id"] == preset_id].iloc[0].to_dict()
    return None

# Seleziona preset per generazione risposta (comune a entrambe le modalit√†)
generation_preset_name = st.selectbox(
    "Seleziona Preset per Generazione Risposta LLM",
    options=preset_display_names,
    index=0 if preset_display_names else None, # Seleziona il primo di default
    key="generation_preset_select",
    help="Il preset API utilizzato per generare la risposta alla domanda."
)
st.session_state.selected_generation_preset_name = generation_preset_name

# Seleziona preset per valutazione (solo per modalit√† LLM)
if st.session_state.test_mode == "Valutazione Automatica con LLM":
    evaluation_preset_name = st.selectbox(
        "Seleziona Preset per Valutazione Risposta LLM",
        options=preset_display_names,
        index=0 if preset_display_names else None, # Seleziona il primo di default
        key="evaluation_preset_select",
        help="Il preset API utilizzato dall'LLM per valutare la similarit√† e correttezza della risposta generata."
    )
    st.session_state.selected_evaluation_preset_name = evaluation_preset_name
else:
    # Per BM25, non c'√® un preset di valutazione LLM
    st.session_state.selected_evaluation_preset_name = None 

show_api_details = st.checkbox("Mostra Dettagli Chiamate API nei Risultati", value=False)

# --- Logica di Esecuzione Test ---
test_mode_selected = st.session_state.test_mode

if test_mode_selected == "Valutazione Automatica con LLM":
    st.header("Esecuzione: Valutazione Automatica con LLM")
    if st.button("üöÄ Esegui Test con LLM", key="run_llm_test_btn"):
        gen_preset_config = get_preset_config_by_name(st.session_state.selected_generation_preset_name)
        eval_preset_config = get_preset_config_by_name(st.session_state.selected_evaluation_preset_name)
        
        if not gen_preset_config or not eval_preset_config:
            st.error("Assicurati di aver selezionato preset validi per generazione e valutazione.")
            st.stop()
            
        with st.spinner("Generazione risposte e valutazione LLM in corso..."):
            results = {}
            for q_id in questions_in_set:
                q_data = get_question_data(q_id)
                if q_data:
                    # Genera risposta di esempio usando LLM
                    generation_output = generate_example_answer_with_llm(q_data['question'], client_config=gen_preset_config, show_api_details=show_api_details)
                    actual_answer = generation_output["answer"]
                    generation_api_details = generation_output["api_details"]
                    
                    if actual_answer is None:
                        # Gestione errore generazione
                        results[q_id] = { 
                            'question': q_data['question'], 
                            'expected_answer': q_data['expected_answer'], 
                            'actual_answer': "Errore Generazione", 
                            'evaluation': {'score':0, 'explanation':'Generazione fallita'},
                            'generation_api_details': generation_api_details # Salva anche se la generazione fallisce
                        }
                        continue
                    
                    evaluation = evaluate_answer(q_data['question'], q_data['expected_answer'], actual_answer, client_config=eval_preset_config, show_api_details=show_api_details)
                    results[q_id] = {
                        'question': q_data['question'], 
                        'expected_answer': q_data['expected_answer'], 
                        'actual_answer': actual_answer, 
                        'evaluation': evaluation, # Questo conterr√† i dettagli API della VALUTAZIONE
                        'generation_api_details': generation_api_details # Dettagli API della GENERAZIONE
                    }
            # Salva e visualizza risultati (omesso per brevit√†, ma simile a prima)
            if results:
                avg_score = sum(r['evaluation']['score'] for r in results.values()) / len(results) if results else 0
                result_data = {
                    'set_name': selected_set['name'], 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'avg_score': avg_score, 'sample_type': 'Generata da LLM', 'method': 'LLM',
                    'generation_preset': gen_preset_config['name'], 'evaluation_preset': eval_preset_config['name'],
                    'questions': results
                }
                result_id = add_test_result(selected_set_id, result_data)
                st.success(f"Test LLM completato! Punteggio medio: {avg_score:.2f}%")
                # ... (visualizzazione risultati) ...

elif test_mode_selected == "Valutazione con BM25":
    st.header("Esecuzione: Valutazione con BM25")
    # Parametri BM25 (k1, b, soglie) ...
    k1_bm25 = st.slider("k1 (BM25)", 0.5, 3.0, 1.5, 0.1, key="bm25_k1_slider")
    b_bm25 = st.slider("b (BM25)", 0.0, 1.0, 0.75, 0.05, key="bm25_b_slider")
    high_threshold_bm25_val = st.slider("Soglia Match Alto (BM25 %)", 50, 100, 80, 1, key="bm25_high_thresh_slider")
    medium_threshold_bm25_val = st.slider("Soglia Match Medio (BM25 %)", 20, 80, 50, 1, key="bm25_medium_thresh_slider")
    
    if st.button("üöÄ Esegui Test con BM25", key="run_bm25_test_btn"):
        gen_preset_config = get_preset_config_by_name(st.session_state.selected_generation_preset_name)
        if not gen_preset_config:
            st.error("Seleziona un preset valido per la generazione delle risposte.")
            st.stop()

        with st.spinner("Generazione risposte e valutazione BM25 in corso..."):
            results = {}
            for q_id in questions_in_set:
                q_data = get_question_data(q_id)
                if q_data:
                    generation_output = generate_example_answer_with_llm(q_data['question'], client_config=gen_preset_config, show_api_details=show_api_details)
                    actual_answer = generation_output["answer"]
                    generation_api_details = generation_output["api_details"]

                    if actual_answer is None:
                        # Log pi√π dettagliato dell'errore per diagnostica
                        st.error(f"Errore nella generazione della risposta per la domanda: {q_data['question'][:50]}...")
                        
                        # Salviamo informazioni pi√π dettagliate nell'oggetto risultati
                        results[q_id] = { 
                            'question': q_data['question'], 
                            'expected_answer': q_data['expected_answer'], 
                            'actual_answer': "Errore Generazione", 
                            'similarity_score': 0, 
                            'match_level': 'basso',
                            'missing_keywords': [], 
                            'extra_keywords': [],
                            'suggestions': "Non √® stato possibile generare una risposta per questa domanda. Verificare le impostazioni del preset API.",
                            'evaluation': {'score': 0, 'explanation': "Errore nella generazione della risposta."},
                            'generation_api_details': generation_api_details # Salva anche se la generazione fallisce
                        }
                        continue
                    
                    similarity_score = calcola_similarita_normalizzata(q_data['expected_answer'], actual_answer, k1=k1_bm25, b=b_bm25)
                    # ... (logica BM25: match_level, missing_keywords, extra_keywords, suggestions) ...
                    match_level = "alto" if similarity_score >= high_threshold_bm25_val else ("medio" if similarity_score >= medium_threshold_bm25_val else "basso")
                    missing_keywords, extra_keywords = analizza_parole_chiave(q_data['expected_answer'], actual_answer)
                    suggestions = genera_suggerimenti(missing_keywords, match_level)

                    results[q_id] = {
                        'question': q_data['question'], 'expected_answer': q_data['expected_answer'], 'actual_answer': actual_answer,
                        'similarity_score': similarity_score, 'match_level': match_level, 
                        'missing_keywords': missing_keywords, 'extra_keywords': extra_keywords, 'suggestions': suggestions,
                        'evaluation': { 'score': similarity_score, 'explanation': suggestions }, # Adattare per coerenza
                        'generation_api_details': generation_api_details # Dettagli API della GENERAZIONE
                    }
            # Salva e visualizza risultati (omesso per brevit√†)
            if results:
                avg_score = sum(r['similarity_score'] for r in results.values()) / len(results) if results else 0
                result_data = {
                    'set_name': selected_set['name'], 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'avg_score': avg_score, 'sample_type': 'Generata da LLM', 'method': 'BM25',
                    'generation_preset': gen_preset_config['name'],
                    'parameters': {'k1': k1_bm25, 'b': b_bm25, 'high_threshold': high_threshold_bm25_val, 'medium_threshold': medium_threshold_bm25_val},
                    'questions': results
                }
                result_id = add_test_result(selected_set_id, result_data)
                st.success(f"Test BM25 completato! Punteggio medio: {avg_score:.2f}%")
                # ... (visualizzazione risultati) ...

# La visualizzazione dei risultati dettagliati √® stata omessa per brevit√† ma dovrebbe essere simile a prima,
# adattata per mostrare i dettagli specifici di LLM vs BM25.
